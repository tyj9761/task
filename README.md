# task
Datawhale/teamlearning-nlp/Emotional_analysis

task0: 第一天的任务是了解Pytorch情感分析的环境配置与基本学习内容。在配置环境的时候接触到了之前没有了解过的torchtext，查阅相关资料后了解到torchtext是一个对数据进行处理的包，并对其用法进行了简单学习。


task1：整个任务是对情感分析这一任务的初步学习。主要设计的是使用RNN做为baseline进行情感分析。
      经过学习了解到，RNN（循环神经网络）每次的输入是一个序列x={x1,x2,...xn},在输入和输出之间有若干的隐藏层。在每一层网络中，当前的隐藏层的输入是由上一个隐藏层的输出和当前的输入组成。所以对序列   模型的表现比较好。
      在对代码进行了复现学习后了解了一般的nlp任务的大致流程包括：
      1.数据预处理。这里使用了torchtext这一个工具，主要使用的是Field类来读取数据，build_vocab来建立词汇表，并且使用内置的迭代器iterator。
      2.构建模型。RNN是一个经典的神经网络模型，所以在构建模型时特别需要注意的就是每一层前后的输出维度。
      3.训练模型。训练模型则使用的是随机梯度下降的方法在每次训练中进行参数更新。
      
      在整个过程中主要遇到的问题是1.配置环境时，使用pip下载torchtext以及spacy模型的时候，自动更新了本地的torch版本，导致与已下载的cuda版本不一致导致GPU无法使用。
      2.在理解RNN模型的代码是发现自己对隐藏层的理解还不是很好。
      
      个人总结：在task1中的学习中让我接触到了使用神经网络进行nlp任务，我会吸取其中的经验，在以后的任务中越做越好。

task2： 本任务是在task1的基础上，使用双向LSTM构建模型。通过之前的学习了解到，由于一般RNN的特性，容易出现梯度消失以及梯度爆炸的问题，而LSTM通过引入门控概念能够很好的解决这些问题。
需要注意的是，在本模型里由于使用了bidirectional，会使得性训练层数非常多，所以需要对模型正则化。关于正则化这个问题我本人并不是很理解，希望之后能够对此有更好的理解。 

task3： 本次任务使用的是与前两次截然不同的模型fasttext，通过论文中的描述了解到，fasttext是一个类似于word2vec中的cbow模型的结构。不同的是fasttext在对输入进行embedding之后对所有embedding进行了平均，从而使模型的训练参数大大减小。
      fasttext还使用了与word2vec想似的层次softmax来降低时间上的复杂度，基本思想是根据类别的频率构造霍夫曼树来代替扁平化的标准softmax。
      整体效果上fasttext在处理情感分析这个任务上的速度非常快，准确度与使用RNN的模型相差无几。

task4： 卷积神经网络在视觉任务上有很广泛的应用，因为其能够准确的进行特征提取，并将表示模块化同时高效地利用数据。不同于图像的像素，大部分NLP任务的输入是句子或文档作为一个矩阵，所以在cnn中的filter是在整个矩阵上滑动而不像图像的像素上是局部移动。所以，这个task对与数据预处理上，因为每个filter的大小就能类似确定查看文本的数量，类似于n-gram，所以不需要再在预处理中加入n-gram的内容。
      经过conv2d卷积层之后在通过一个激活函数得到的就是经过处理之后的数据，由于是在整个矩阵上移动，所以不需要stride就可以直接输入池化层。后面的操作与前面的任务大致相同。
      
